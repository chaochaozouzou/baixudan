{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying actual/simulated acceleration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we will use a one-dimensional convolutional network to classify actual and simulated acceleration data, and compare it to other models (logistic regression, svm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading/splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load the actual/simulation datasets, get their lengths, and print first few lines of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows of actual data: 1600\n",
      "# rows of simulated data: 1600\n",
      "\n",
      "first 5 lines of actual data:\n",
      "      x     y     z\n",
      "0 -2.32 -0.86  9.13\n",
      "1 -2.71 -1.37  8.75\n",
      "2 -3.03 -1.70  8.37\n",
      "3 -3.14 -1.92  8.14\n",
      "4 -2.96 -1.99  8.47\n",
      "\n",
      "first 5 lines of simulated data:\n",
      "          x         y         z\n",
      "0 -3.850784 -0.085201 -1.490804\n",
      "1  4.038864 -1.818646 -2.300385\n",
      "2 -1.056220 -0.847621  0.738123\n",
      "3  4.368797 -0.950352  0.329629\n",
      "4  4.805529 -1.561125  0.697635\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('data/data_good.json', 'rb') as f:\n",
    "    raw1 = f.readlines()\n",
    "df1 = pd.read_json(raw1[0])\n",
    "with open('data/simulation.json', 'rb') as f:\n",
    "    raw2 = f.readlines()\n",
    "df2 = pd.read_json(raw2[0])\n",
    "print('# rows of actual data:', len(df1))\n",
    "print('# rows of simulated data:', len(df2))\n",
    "print('')\n",
    "print('first 5 lines of actual data:')\n",
    "print(df1.head())\n",
    "print('')\n",
    "print('first 5 lines of simulated data:')\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then concatenate the datasets, cast the concatenated acceleration data to numpy array for easier processing, create labels from it, and divide acceleration data and labels into training and testing sets with test ratio 0.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pd.concat([df1, df2]))\n",
    "y1 = np.zeros(len(df1.index))\n",
    "y2 = np.ones(len(df2.index))\n",
    "y = np.concatenate([y1, y2])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-dimensional convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first construct the 1d conv net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "cnn = keras.Sequential([\n",
    "    keras.layers.Conv1D(filters=50, kernel_size=2, input_shape=(None, 3), padding='same'), \n",
    "    keras.layers.Dense(2, activation='sigmoid')\n",
    "])\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, neural network models are constructed layer by layer, and the first layer in our model is a conv1d layer taking 3d tensors as inputs, so we need to reshape our input data. Say that our input data has dimension $m \\cdot 3$ ($m$ rows of 3-dimensional acceleration data). The general idea is that we want to run convolution on every few rows of our original dataset, and in order for that to work, we need to reshape our data to dimension $m \\cdot 1 \\cdot 3$ and match it to filters of size $ n \\cdot 3$, where $n$ is the number of rows we want to consider together each time. \n",
    "\n",
    "Note that the output of our model has shape $m \\cdot 1 \\cdot 2$, where each row specifies probabilities that the sample belongs to the 2 categories, so we need to convert/reshape our label. Say that our label has dimension $m \\cdot 1$. So we will first convert our label to one-hot vectors for it to have shape $m \\cdot 2$, and then reshape it to dimension $m \\cdot 1 \\cdot 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = np.array(X_train)\n",
    "X_test_cnn = np.array(X_test)\n",
    "\n",
    "idx_train = [int(idx) for idx in y_train-1]\n",
    "idx_test = [int(idx) for idx in y_test-1]\n",
    "y_train_cnn = np.zeros((len(y_train), 2))\n",
    "y_train_cnn[np.arange(len(y_train)), idx_train] = 1\n",
    "y_test_cnn = np.zeros((len(y_test), 2))\n",
    "y_test_cnn[np.arange(len(y_test)), idx_test] = 1\n",
    "\n",
    "X_train_cnn = X_train_cnn.reshape(X_train_cnn.shape[0], 1, X_train_cnn.shape[1])\n",
    "X_test_cnn = X_test_cnn.reshape(X_test_cnn.shape[0], 1, X_test_cnn.shape[1])\n",
    "y_train_cnn = y_train_cnn.reshape(y_train_cnn.shape[0], 1, y_train_cnn.shape[1])\n",
    "y_test_cnn = y_test_cnn.reshape(y_test_cnn.shape[0], 1, y_test_cnn.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the 1d conv net performs on this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1568 samples, validate on 672 samples\n",
      "Epoch 1/10\n",
      "1568/1568 [==============================] - 0s 172us/step - loss: 0.4895 - acc: 0.7274 - val_loss: 0.3473 - val_acc: 0.8147\n",
      "Epoch 2/10\n",
      "1568/1568 [==============================] - 0s 23us/step - loss: 0.3042 - acc: 0.8501 - val_loss: 0.2771 - val_acc: 0.8728\n",
      "Epoch 3/10\n",
      "1568/1568 [==============================] - 0s 32us/step - loss: 0.2412 - acc: 0.9129 - val_loss: 0.2191 - val_acc: 0.9204\n",
      "Epoch 4/10\n",
      "1568/1568 [==============================] - ETA: 0s - loss: 0.2145 - acc: 0.968 - 0s 28us/step - loss: 0.1878 - acc: 0.9550 - val_loss: 0.1692 - val_acc: 0.9606\n",
      "Epoch 5/10\n",
      "1568/1568 [==============================] - 0s 28us/step - loss: 0.1427 - acc: 0.9799 - val_loss: 0.1288 - val_acc: 0.9829\n",
      "Epoch 6/10\n",
      "1568/1568 [==============================] - 0s 27us/step - loss: 0.1073 - acc: 0.9917 - val_loss: 0.0974 - val_acc: 0.9933\n",
      "Epoch 7/10\n",
      "1568/1568 [==============================] - 0s 27us/step - loss: 0.0804 - acc: 0.9955 - val_loss: 0.0744 - val_acc: 0.9985\n",
      "Epoch 8/10\n",
      "1568/1568 [==============================] - 0s 29us/step - loss: 0.0612 - acc: 0.9984 - val_loss: 0.0572 - val_acc: 0.9993\n",
      "Epoch 9/10\n",
      "1568/1568 [==============================] - 0s 29us/step - loss: 0.0473 - acc: 0.9997 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1568/1568 [==============================] - 0s 29us/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 1.0000\n",
      "960/960 [==============================] - 0s 11us/step\n",
      "loss: 0.03783066018174092\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(X_train_cnn, y_train_cnn, validation_split=0.3, epochs=10)\n",
    "loss, acc = cnn.evaluate(X_test_cnn, y_test_cnn)\n",
    "print('loss:', loss)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/validation/test accuracies quickly rise to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with logistic regression/svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then fit logistic regression/svm on the same training/testing sets for comparison. Besides accuracy, f-score is also printed to check behavior of these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "f-score for two categories: [1. 1.]\n",
      "accuracy: 1.0\n",
      "f-score for two categories: [1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xudanb\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\xudanb\\AppData\\Local\\Continuum\\anaconda3\\envs\\venv\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('accuracy:', accuracy_score(y_test, lr.predict(X_test)))\n",
    "print('f-score for two categories:', f1_score(y_test, lr.predict(X_test), average=None))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "sv = SVC().fit(X_train, y_train)\n",
    "print('accuracy:', accuracy_score(y_test, sv.predict(X_test)))\n",
    "print('f-score for two categories:', f1_score(y_test, sv.predict(X_test), average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then easy to see that the data can be easily separated by simple models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That our data can be separated by simple models doesn't necessarily mean our 1d conv net is useless. We do, however, need to run it on a different set of data to better measure its performance.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
